rate = rate/(prop^lr)
print(rate)
}
return(list(model_interval, prop, pred_spread))
}
results <- optimize_intervals(get_weights(model), predictors = predictors, ori_spread = ori_spread)
create_interval_model <- function(weights, rate){
test_dropout <-  layer_dropout(rate = rate)
input <- layer_input(shape = dim(predictors)[2])
output <- input %>%
layer_dense(units = 128, activation = "relu") %>%
test_dropout(training = T) %>%
layer_dense(units = 128,activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
model_interval <- keras_model(input,output)
# Compile model
model_interval %>% compile(
loss = "mse",
optimizer = optimizer_adam(), metrics = c("mae"))
set_weights(model_interval, weights = weights)
model_interval
}
optimize_intervals <- function(weights, predictors, ori_spread, iterations = 100){
rate <- 0.1
sigma_diff <- 1
lr <- 1
while (sigma_diff > 0.05) {
model_interval <- create_interval_model(weights, rate)
interval_data <- matrix(nrow = dim(predictors)[1], ncol = iterations)
for (i in 1:iterations)
interval_data[,i]  <- model_interval %>% predict(predictors)
dat <- apply(interval_data, 1, function(x) x - median(x))
pred_spread <- matrix(dat, nrow = length(dat))
prop <- (mean(abs(pred_spread))/mean(abs(ori_spread)))
if (exists("sameness")) {
if (sameness != (prop > 1)) {
lr <- lr*0.6
}
}
sameness <- prop > 1
rate = rate/(prop^lr)
print(rate)
sigma_diff <- prop-1
print(paste("sigma_diff", sigma_diff))
}
return(list(model_interval, prop, pred_spread))
}
results <- optimize_intervals(get_weights(model), predictors = predictors, ori_spread = ori_spread)
sameness
if (exists("sameness")) {remove("sameness")}
results
results[[2]]
pred_spread <- results[[3]]
origin <- c(rep(T, length(ori_spread)),  rep(F, length(pred_spread)))
spread <- rbind(ori_spread, pred_spread)
spread <- tibble(diff = spread[,1], origin = origin)
spread %>% ggplot(aes(diff, fill = origin)) +
geom_histogram(aes(y = after_stat(ncount)), position = "identity", alpha = 0.6)
create_interval_model <- function(weights, rate){
test_dropout <-  layer_dropout(rate = rate)
input <- layer_input(shape = dim(predictors)[2])
output <- input %>%
layer_dense(units = 128, activation = "relu") %>%
test_dropout(training = T) %>%
layer_dense(units = 128,activation = "relu") %>%
test_dropout(training = T) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
model_interval <- keras_model(input,output)
# Compile model
model_interval %>% compile(
loss = "mse",
optimizer = optimizer_adam(), metrics = c("mae"))
set_weights(model_interval, weights = weights)
model_interval
}
optimize_intervals <- function(weights, predictors, ori_spread, iterations = 100){
rate <- 0.1
sigma_diff <- 1
lr <- 1
if (exists("sameness")) {remove("sameness")}
while (sigma_diff > 0.05) {
model_interval <- create_interval_model(weights, rate)
interval_data <- matrix(nrow = dim(predictors)[1], ncol = iterations)
for (i in 1:iterations)
interval_data[,i]  <- model_interval %>% predict(predictors)
dat <- apply(interval_data, 1, function(x) x - median(x))
pred_spread <- matrix(dat, nrow = length(dat))
prop <- (mean(abs(pred_spread))/mean(abs(ori_spread)))
if (exists("sameness")) {
if (sameness != (prop > 1)) {
lr <- lr*0.6
}
}
sameness <- prop > 1
rate = rate/(prop^lr)
print(rate)
sigma_diff <- prop-1
print(paste("sigma_diff", sigma_diff))
}
return(list(model_interval, prop, pred_spread))
}
results <- optimize_intervals(get_weights(model), predictors = predictors, ori_spread = ori_spread)
pred_spread <- results[[3]]
origin <- c(rep(T, length(ori_spread)),  rep(F, length(pred_spread)))
spread <- rbind(ori_spread, pred_spread)
spread <- tibble(diff = spread[,1], origin = origin)
spread %>% ggplot(aes(diff, fill = origin)) +
geom_histogram(aes(y = after_stat(ncount)), position = "identity", alpha = 0.6)
create_interval_model <- function(weights, rate){
test_dropout <-  layer_dropout(rate = rate)
input <- layer_input(shape = dim(predictors)[2])
output <- input %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dense(units = 128,activation = "relu") %>%
test_dropout(training = T) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
model_interval <- keras_model(input,output)
# Compile model
model_interval %>% compile(
loss = "mse",
optimizer = optimizer_adam(), metrics = c("mae"))
set_weights(model_interval, weights = weights)
model_interval
}
optimize_intervals <- function(weights, predictors, ori_spread, iterations = 100){
rate <- 0.1
sigma_diff <- 1
lr <- 1
if (exists("sameness")) {remove("sameness")}
while (sigma_diff > 0.05) {
model_interval <- create_interval_model(weights, rate)
interval_data <- matrix(nrow = dim(predictors)[1], ncol = iterations)
for (i in 1:iterations)
interval_data[,i]  <- model_interval %>% predict(predictors)
dat <- apply(interval_data, 1, function(x) x - median(x))
pred_spread <- matrix(dat, nrow = length(dat))
prop <- (mean(abs(pred_spread))/mean(abs(ori_spread)))
if (exists("sameness")) {
if (sameness != (prop > 1)) {
lr <- lr*0.6
}
}
sameness <- prop > 1
rate = rate/(prop^lr)
print(rate)
sigma_diff <- prop-1
print(paste("sigma_diff", sigma_diff))
}
return(list(model_interval, prop, pred_spread))
}
results <- optimize_intervals(get_weights(model), predictors = predictors, ori_spread = ori_spread)
pred_spread <- results[[3]]
origin <- c(rep(T, length(ori_spread)),  rep(F, length(pred_spread)))
spread <- rbind(ori_spread, pred_spread)
spread <- tibble(diff = spread[,1], origin = origin)
spread %>% ggplot(aes(diff, fill = origin)) +
geom_histogram(aes(y = after_stat(ncount)), position = "identity", alpha = 0.6)
source('~/graph_analysis_tools.R', echo = F)
predictors[1:100, ]
model_interval <- results[[1]]
test_preds ,-  predictors[1:100, ]
test_predicts <- predictors[1:100, ]
model_interval <- results[[1]]
test_predicts <- predictors[1:100, ]
iterations <- 100
interval_data <- matrix(nrow = dim(test_predicts)[1], ncol = iterations)
for (i in 1:iterations)
interval_data[,i]  <- model_interval %>% predict(test_predicts)
confidence_interval <- 0.6
calc_conf_interval <- function(x){
quantile(x, c(0.5 - confidence_interval/2, 0.5 + confidence_interval/2 ))
}
calc_conf_interval
calc_conf_interval <- apply(interval_data, 1, function(x){
quantile(x, c(0.5 - confidence_interval/2, 0.5 + confidence_interval/2 ))
})
calc_conf_interval
dim(calc_conf_interval)
calc_conf_interval <- t(calc_conf_interval)
honest_pred <- model %>% predict(test_predicts)
target_test <- target[1:100]
testing_data <- tibble(true = target_test, pred = honest_pred,
yp = calc_conf_interval[,1], ym = calc_conf_interval[,2] )
testing_data %>% ggplot(aes(true, test)) + geom_point() + geom_pointrange(aes(ymin = yp, ymax = ym))
confidence_interval <- 0.6
calc_conf_interval <- apply(interval_data, 1, function(x){
quantile(x, c(0.5 - confidence_interval/2, 0.5 + confidence_interval/2 ))
})
calc_conf_interval <- t(calc_conf_interval)
honest_pred <- model %>% predict(test_predicts)
target_test <- target[1:100]
testing_data <- tibble(true = target_test, pred = honest_pred,
yp = calc_conf_interval[,1], ym = calc_conf_interval[,2] )
testing_data %>% ggplot(aes(true, pred)) + geom_point() + geom_pointrange(aes(ymin = yp, ymax = ym))
testing_data %>% ggplot(aes(true, pred)) + geom_point() +
geom_pointrange(aes(ymin = yp, ymax = ym)) + geom_abline(slope = 1, col = "red", size = 2)
library(tidyverse)
library(igraph)
library(ggplot2)
library(ggraph)
library(magrittr)
library(tidymodels)
library(foreach)
library(doParallel)
source('~/studia/zaklad/EC_rainbow/model_creation/graph_analysis_tools.R', echo = F)
lapply(g_comps, function(z)  lapply(z, lenght(V(x)) ))
lapply(g_comps, function(z)  lapply(z, length(V(x)) ))
lapply(g_comps, function(z)  lapply(z, length(V) ))
lapply(g_comps, function(z)  lapply(z, function(x), length(V(x)) ))
lapply(g_comps, function(z) lapply(z, function(x) length(V(x)) ) )
create_interval_model <- function(weights, rate){
test_dropout <-  layer_dropout(rate = rate)
input <- layer_input(shape = dim(predictors)[2])
output <- input %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dense(units = 128,activation = "relu") %>%
test_dropout(training = T) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
model_interval <- keras_model(input,output)
# Compile model
model_interval %>% compile(
loss = "mse",
optimizer = optimizer_adam(), metrics = c("mae"))
set_weights(model_interval, weights = weights)
model_interval
}
optimize_intervals <- function(weights, predictors, ori_spread, iterations = 100){
rate <- 0.1
sigma_diff <- 1
lr <- 1
if (exists("sameness")) {remove("sameness")}
while (sigma_diff > 0.05) {
model_interval <- create_interval_model(weights, rate)
interval_data <- matrix(nrow = dim(predictors)[1], ncol = iterations)
for (i in 1:iterations)
interval_data[,i]  <- model_interval %>% predict(predictors)
dat <- apply(interval_data, 1, function(x) x - median(x))
pred_spread <- matrix(dat, nrow = length(dat))
prop <- (mean(abs(pred_spread))/mean(abs(ori_spread)))
if (exists("sameness")) {
if (sameness != (prop > 1)) {
lr <- lr*0.6
}
}
sameness <- prop > 1
rate = rate/(prop^lr)
print(rate)
sigma_diff <- abs(prop - 1)
print(paste("sigma_diff", sigma_diff))
}
return(list(model_interval, prop, pred_spread))
}
generate_data_fast <- function(graph, simulation_parameters, alpha =0.7, n_try = 3){
source('~/studia/zaklad/EC_rainbow/model_creation/graph_analysis_tools.R', echo = F)
fast_outs = list()
for (i in 1:length(alpha)) {
fast_outs[[i]] <- fast_assorts_p_1(graph, alpha = alpha[i])
}
cl <- parallel::makeCluster(7)
doParallel::registerDoParallel(cl)
init_size <- length(V(graph))
date()
res <- foreach(sim_params = iter(simulation_parameters, by = 'row'), .export = c("graph", "init_size", "fast_outs",
"mix_mat", "simulate_expansion_of_fraction_cheap",
"fast_assorts_p_2"),
.packages = c("igraph")) %:% foreach(try = 1:n_try) %dopar% {
survivor_f <- sim_params[1]
expanding_f <- sim_params[2]
set.seed(200 + survivor_f + 10*expanding_f + try*100)
cols <- sample(1:3, init_size, replace = T)
test_cheap <- simulate_expansion_of_fraction_cheap(graph = graph,
survivor_fraction = survivor_f,
expanding_fraction = expanding_f, cols)
n_surv <- ceiling(init_size*survivor_f)
n_exp <- ceiling(n_surv*expanding_f)
asses <- list()
for (i in 1:length(fast_outs)) {
ass <- fast_assorts_p_2(test_cheap$final, fast_outs[[i]][[1]], fast_outs[[i]][[2]], fast_outs[[i]][[3]])
asses[[i]] <- ass
}
list(asses, c(n_surv, n_exp))
}
date()
parallel::stopCluster(cl)
res_ori <- res
res <- unlist(recursive = F, res)
return(res)
}
compile_assortativity_data <- function(assortativities_list){
r <- (seq(0.1, 0.6, by = 0.1))^1.8
q_probes <- sort(c(r,0.5,1 - r))
hists <- map(assortativities_list, function(z){
z <- z[[1]]
vals <- lapply(z[1:2], function(x){
mean = mean(x)
sd = sd(x)
skew = moments::skewness(x)
kurt = moments::kurtosis(x)
quants <- quantile(x, probs = q_probes)
c1 <- c(mean, sd, skew, kurt)
names(c1) <- c("mean", "sd", "skew", "kurt")
c(c1, quants)
})
unlist(vals)
})
wh_cors <- map_dfr(assortativities_list, function(z){
z <- z[[1]]
mat <- matrix(unlist(z[1:2]), nrow = 3)
mat <- t(mat)
v_cors <- cor(mat, method = "s")
vals <- v_cors[upper.tri(v_cors)]
names(vals) <- 1:length(vals)
vals
})
assortativities_frame <- matrix(unlist(hists), ncol = length(hists))
assortativities_frame <- t(assortativities_frame)
colnames(assortativities_frame) <- make.unique(names(hists[[1]]), sep = "_")
colnames(wh_cors) <- c("one", "two", "three")
comb_frame <- bind_cols(as_tibble(assortativities_frame), wh_cors)
}
# bottom of curvature
graph <-  g_comps[[1]][[2]]
survivor_fraction <- runif(6000, 0.6, 0.98)
# expanding_fraction <- seq(0.001, 0.6, length.out = 60)
expanding_fraction <-  runif(6000, 0.005, 0.6)
sim_params <- cbind(survivor_fraction, expanding_fraction)
smal_res_04_07 <- generate_data_fast(graph, sim_params, alpha = c(0.4, 0.7), n_try = 1)
res <- smal_res_04_07
init_size <- length(V(graph))
params_df <- map_dfr(res, function(x){
s <- x[[2]][1]
e <- x[[2]][2]
list(surv =  s/init_size, pseudo_eras = log2((init_size - s)/e + 1),  expan = e/s )
})
# predict survival successful model !!!!
params_df <- map_dfr(res, function(x){
s <- x[[2]][1]
e <- x[[2]][2]
list(surv =  s/init_size, pseudo_eras = log2((init_size - s)/e + 1),  expan = e/s )
})
library(keras)
library(tensorflow)
library(caret)
# for double assortativity
assorts_data <- compile_assortativity_data(res)
assorts_data <- as.matrix(assorts_data)
rot_mat <- prcomp(assorts_data, rank. = 13, scale. = T)$rot
predictors <- assorts_data %*% rot_mat
comb_frame <- cbind(params_df ,predictors)
indexes = createDataPartition(params_df$surv, p = .7, list = F)
# predictors <- sub_features
predictors <- as.data.frame(comb_frame %>% select(-surv, -pseudo_eras, -expan))
predictors <- as.matrix(predictors)
target <- as.matrix(comb_frame$surv, ncol = 1)
xtrain = predictors[indexes,]
indexes_val = createDataPartition(1:length(indexes), p = .7, list = F)
xval = xtrain[-indexes_val,]
xtrain = xtrain[indexes_val,]
xtest = predictors[-indexes,]
# targets <- targets
ytrain = target[indexes]
yval = ytrain[-indexes_val]
ytrain = ytrain[indexes_val]
ytest = target[-indexes]
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = dim(predictors)[2], units = 128, activation = "relu") %>%
layer_dense(units = 128,activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(rate = 0.05) %>%
layer_dense(units = 1, activation = "linear")
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(), metrics = c("mae"))
history <- model %>% fit(x = xtrain, y = ytrain, epochs = 100, verbose = 1, validation_data = list(xval, yval),
callbacks = list(
callback_early_stopping(patience = 20, restore_best_weights = T),
callback_reduce_lr_on_plateau(factor = 0.3))
)
scores = model %>% evaluate(xtest, ytest, verbose = 0)
print(scores)
pred <- model %>% predict(xtest, verbose = 0)
(cor(ytest, pred))^2
whole_ds_pred <- model %>% predict(predictors)
# trans_w_pred <- tibble(surv_p = whole_ds_pred*sds[1] + means[1])
trans_w_pred <- tibble(surv_p = whole_ds_pred)
plot_d <- bind_cols(surv = target, trans_w_pred, group = 1:dim(whole_ds_pred)[1] %in% indexes[indexes_val] )
plot_d %>%
ggplot(aes(surv, surv_p)) + geom_point(  aes(col = group) ,alpha = 0.15, shape = 16 ) + geom_smooth(aes(group = as.factor(group))) +
geom_abline(slope = 1, col = "red", size = 2)
plot_d %>% ggplot(aes(  (surv - surv_p))) + geom_histogram(  aes(fill = group, y = after_stat(ncount)), position = "identity", alpha = 0.5)
results <- optimize_intervals(get_weights(model), predictors = predictors, ori_spread = ori_spread)
pred_spread <- results[[3]]
origin <- c(rep(T, length(ori_spread)),  rep(F, length(pred_spread)))
spread <- rbind(ori_spread, pred_spread)
spread <- tibble(diff = spread[,1], origin = origin)
spread %>% ggplot(aes(diff, fill = origin)) +
geom_histogram(aes(y = after_stat(ncount)), position = "identity", alpha = 0.6)
model_interval <- results[[1]]
test_predicts <- predictors[1:100, ]
iterations <- 100
interval_data <- matrix(nrow = dim(test_predicts)[1], ncol = iterations)
for (i in 1:iterations)
interval_data[,i]  <- model_interval %>% predict(test_predicts)
confidence_interval <- 0.6
calc_conf_interval <- apply(interval_data, 1, function(x){
quantile(x, c(0.5 - confidence_interval/2, 0.5 + confidence_interval/2 ))
})
calc_conf_interval <- t(calc_conf_interval)
honest_pred <- model %>% predict(test_predicts)
target_test <- target[1:100]
testing_data <- tibble(true = target_test, pred = honest_pred,
yp = calc_conf_interval[,1], ym = calc_conf_interval[,2] )
testing_data %>% ggplot(aes(true, pred)) + geom_point() +
geom_pointrange(aes(ymin = yp, ymax = ym)) + geom_abline(slope = 1, col = "red", size = 2)
ass_ori_04 <- assortativity_local_par(graph, get.vertex.attribute(graph)$colour, alpha = 0.4)
ass_ori_07 <- assortativity_local_par(graph, get.vertex.attribute(graph)$colour, alpha = 0.7)
ass_ori_data <- list(list(list(ass_ori_04, ass_ori_07)))
assorts_data_ori <- compile_assortativity_data(ass_ori_data)
assorts_data_ori <- as.matrix(assorts_data_ori)
predictors_ori <- assorts_data_ori %*% rot_mat
tru_predict <- model %>% predict(predictors_ori)
iterations <- 200
interval_ori <- matrix(nrow = dim(predictors_ori)[1], ncol = iterations)
for (i in 1:iterations)
interval_ori[,i]  <- model_interval %>% predict(predictors_ori)
confidence_interval <- 0.6
conf_interval_ori <- quantile(interval_ori, c(0.5 - confidence_interval/2, 0.5 + confidence_interval/2) )
conf_interval_ori
tru_predict
# bottom of curvature
survivor_fraction <- round(tru_predict * init_size, 0) / init_size
survivor_fraction <- rep(survivor_fraction, 3000)
# expanding_fraction <- seq(0.001, 0.6, length.out = 60)
expanding_fraction <-  runif(3000, 1/init_size, 0.6)
sim_expansion <- cbind(survivor_fraction, expanding_fraction)
res_04_07_expansion <- generate_data_fast(graph, sim_expansion, alpha = c(0.4, 0.7), n_try = 1)
res <- res_04_07_expansion
params_df_expan <- map_dfr(res, function(x){
s <- x[[2]][1]
e <- x[[2]][2]
list(surv =  s/init_size, pseudo_eras = log2((init_size - s)/e),  expan = e/s )
})
data_expanding <- compile_assortativity_data(res)
data_expanding <- as.matrix(data_expanding)
rot_mat_expan <- prcomp(data_expanding, rank. = 13, scale. = T)$rot
predictors_expan <- data_expanding %*% rot_mat_expan
expan_frame <- cbind(params_df_expan ,predictors_expan)
new_cors <- cor(expan_frame, method = "k")
heatmap(x = new_cors, scale = "none", Colv = NA, Rowv = NA)
indexes = createDataPartition(params_df_expan$expan, p = .85, list = F)
predictors <- as.data.frame(expan_frame %>% select(-surv, -pseudo_eras, -expan))
predictors <- as.matrix(predictors)
target <- as.matrix(expan_frame$pseudo_eras, ncol = 1)
xtrain = predictors[indexes,]
indexes_val = createDataPartition(1:length(indexes), p = .85, list = F)
xval = xtrain[-indexes_val,]
xtrain = xtrain[indexes_val,]
xtest = predictors[-indexes,]
# targets <- targets
ytrain = target[indexes]
yval = ytrain[-indexes_val]
ytrain = ytrain[indexes_val]
ytest = target[-indexes]
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = dim(predictors)[2], units = 128, activation = "relu") %>%
layer_dense(units = 128,activation = "relu") %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(rate = 0.05) %>%
layer_dense(units = 1, activation = "linear")
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(), metrics = c("mae"))
history <- model %>% fit(x = xtrain, y = ytrain, epochs = 100, verbose = 1, validation_data = list(xval, yval),
callbacks = list(
callback_early_stopping(patience = 20, restore_best_weights = T),
callback_reduce_lr_on_plateau(factor = 0.3))
)
scores = model %>% evaluate(xtest, ytest, verbose = 0)
print(scores)
pred <- model %>% predict(xtest, verbose = 0)
(cor(ytest, pred))^2
whole_ds_pred <- model %>% predict(predictors)
# trans_w_pred <- tibble(surv_p = whole_ds_pred*sds[1] + means[1])
trans_w_pred <- tibble(surv_p = whole_ds_pred)
plot_d <- bind_cols(surv = target, trans_w_pred, group = 1:dim(whole_ds_pred)[1] %in% indexes[indexes_val] )
plot_d %>%
ggplot(aes(surv, surv_p)) + geom_point(  aes(col = group) ,alpha = 0.15, shape = 16 ) +
geom_smooth(aes(group = as.factor(group)), span = 2) +
geom_abline(slope = 1, col = "red", size = 2)
plot_d %>% ggplot(aes(  (surv - surv_p))) +
geom_histogram(  aes(fill = group, y = after_stat(ncount)), position = "identity", alpha = 0.5)
# log2((init_size - s)/e + 1) = era
# 2^era = (1-surv_r)/e + 1
# e = surv_r/2^era -1
log2((1 - 0.848)/0.321 + 1)
predictors_ori_expan <- assorts_data_ori %*% rot_mat_expan
tru_predict_expan <- model %>% predict(predictors_ori_expan)
tru_predict_expan
(1 - tru_predict)/(2^tru_predict_expan)
save.image("~/studia/zaklad/EC_rainbow/model_creation/.RData")
